In the digital world of today, malware is still a massive and growing problem. While back in the day it was used to annoy users and system administrators, nowadays it is used for extortion, cyber espionage and surveillance by criminal groups and rivalling governments. One of the main causes of getting infected with malware is a drive-by download while visiting a normal day-to-day website because, for example, the website got hacked and infected. 

In many cases \cite{proofpoint,foxittelegraaf,foxityahoo} however, it was not the actual website but one of the advertisement networks that was infiltrated and which subsequently started serving malicious code hidden in innocently looking advertisement code. This is also called malvertising \cite{Li2012}.

National CERT organisations are interested in an early detection of such threats. While automated systems to scan websites already exist, like Cuckoo\footnote{http://cuckoosandbox.org} and Anubis\footnote{http://anubis.iseclab.org}, one of the main downsides is the time needed to analyse multiple websites.

The goal of this research project is to develop an algorithm that makes it possible to examine multiple websites for the existence of malware using the same computer system and at the same time, increasing the speed at which detection can occur. The effectiveness of this algorithm will be proved by implementing it in a proof of concept.

\subsection{Research Question}

In cooperation with the Dutch National Cyber Security Center (NCSC-NL), our research project focuses on the question:

\textit{How can we concurrently visit multiple URLs and still be able to determine which URL was responsible for malicious activities?}

To answer the research question, multiple sub-questions have been formulated:

\begin{itemize}
\item Which techniques are used by web browsers to make concurrently visiting multiple URLs possible?
\item Which APIs are used by web browsers to make HTTP requests and retrieve webpages?
\item How can an HTTP request be correlated to its source URL without the modification of the used web browser?
\item Which additional information sources (from the client or its environment), besides what was used to correlate HTTP requests, can be used to make the tracking of malware to its source URL easier?
%What extra information from the client's (running) machine can be used to augment the information gained from network traffic to make the tracking of malware to its source URL easier?
\end{itemize}

\pagebreak
\restoregeometry

% \subsection{Motivation?}

\subsection{Related work}

The growing importance of malware resulted in many research projects in the last few years. The detection and analysis of malware has been researched from several different angles \cite{auto_malware,Chang2013} and resulted in many proposed static and dynamic analysis techniques.

In 2013, Le \textit{et al.} \cite{Le2013} presented a framework that describes the common stages and characteristics of a drive-by download attack. They described four stages from placing the malicious content on a webpage until the execution of the malicious activity.

In 2011, a paper from Canali \textit{et al.} \cite{Canali2011} was released about the problematic performance of dynamic analysis and with a solution proposed in the form of ``Prophiler''. Prophiler is a filter that deploys static analysis techniques and that is able to reduce the load with more than 85\% compared to dynamic analysis. This with a comparable amount of false negatives.

In the same year Rajab \textit{et al.} \cite{Rajab11trendsin} gave an overview of the trends regarding web malware detection and how the malware tries to circumvent the detection. This research focused on the advantages and disadvantages of four techniques: Virtual Machine honeypots, Browser Emulation honeypots, Classification based on Domain Reputation and Anti-Virus Engines.

A different approach was taken by Rossow \textit{et al.} \cite{Rossow2011}; Cortjens and El Yassem \cite{Cortjens2012}; Kinkhorst and Van Kleij \cite{Kinkhorst2009}. During multiple research projects they focused on the ability to detect and identify malware on the network layer.

The usage of graphs to detect malware has been proposed before. Park and Reeves proposed \cite{Park2011} the usage of graphs of system calls to detect the similarities and differences in behaviour between the variations of a malware family. By focusing on the common subgraph, new variants can be detected and categorized without prior knowledge of their existence. A recent paper \cite{Wuchner2014} from W\"{u}chner \textit{et al.} described the usage of generating graphs from API calls for a heuristic based malware detection system.

The predecessor of NCSC-NL started in 2007 with the development of their own system, the Honeyspider network \cite{honeyspider}, for the dynamic analysis of websites. This system crawled the biggest and most important websites of the Netherlands on a daily base. The downside of this system is that it requires a lot of maintenance and therefore it started to become outdated.

\iffalse
\subsection{Scope}

\todo{Zelfstandiger maken}

In this research project is an an algorithm created that allows multiple URLs to be opened at the same time while still being able to track all further interaction, such as unexpected HTTP requests and other malicious activity, and link them to the original request/URL. To prove that the algorithm is something feasible, a proof of concept of the algorithm has been implemented on top of the Cuckoo Sandbox.

The goal during this research project was to make the algorithm fully platform agnostic, however, several technical challenges prevented this. For this reason we limited our self to Windows 7 with version 8 of the Internet Explorer browser and have we the identified issues described.

The detection and identification of malicious behaviour was not part of this project. For our PoC we sticked to the detection of a well-known older and still to be determined malware family which existence is easy to detect on the system. 

\subsection{Ethical issues}

% Zinnen herschrijven die naar de toekomst verwijzen.
Our research contains no major ethical issues as it does not include working with personally identifiable information. Malware, if any, will be run in a controlled virtual environment. After every testrun the virtual machine will be automatically destroyed.

\fi
