% Intro is overbodig, slechts eenmaal in begin van hoofdstuk doen

\subsubsection{Generic algorithm}

The algorithm proposed takes as input a list of URLs, visits them and returns, as output, the URL which behaves abnormally. To be able to visit those URLs concurrently, and thus provide large-scale analysis, the algorithm has to be capable of linking events to a URL from the input. This is done by monitoring the network traffic and additional information (explained in \ref{algo2}\todo{ref updaten}). After gathering this data, the events are stored in a graph after which analysis can be run to detect drive-by downloads.

\begin{enumerate}
\item Visit.
\item Process.\todo{Zet in het midden + label / caption}
\item Analyze.
\item Report.
\end{enumerate}

\subsubsection{Prerequisites and considerations\todo{Considerations vervangen door constraints?}}

To correctly associate events with a browsing context and analyze the machine's behavior after visiting a URL, monitoring is needed. Monitoring can be split up in two distinct parts, namely the network traffic and all extra information that is needed to associate malicious behavior with a browsing context.\\

To track drive-by downloads, we consider process spawns, shell commands and file operations\footnote{For Microsoft Windows, the Registry should also be considered.} as valid information to track. Of course these operations should only be tracked for processes who are child processes of the browseri or the browser process itself.\\
\todo{Extra informatie is een research question! Leg uit}

The question of course is how this information can be retrieved and on which level we have to work. Turns out that although most information can be intercepted on the operating system level, most of the time the operation/API call misses some information to be linked to a browsing context and thus to a URL. Making it often necessary to actually work at a browser level to be able to know from which tab/window the operation came.% Most, if not all, information can be retrieved on the operating system level. Only the 



One of the major considerations that was made, is the fact that the algorithm should work on multiple operating systems and multiple web browsers. The second constraint is a reasonable runtime, meaning the algorithm should run in an acceptable time on normal commodity hardware.

\subsubsection{Step 1: Visit}

After the monitoring is set up, potentially malicious websites can be visited. Due to the monitoring, the behavior of a website can be monitored in such a way that multiple websites can be visited at one given time. This concurrency is a major speed improvement compared to old systems like HoneySpider Network.


\subsubsection{Step 2: Process}

\todo{Low-level -> high level is niet noodzakelijk}
Tracking the system calls the browser makes, generates a significant amount of data which is not entirely useful in itself for the analyze phase. Therefor low-level calls are grouped into high-level events. As an example, consider the writing of a file. To write to a file, a file handle/descriptor has to be made or opened. Then one or more write calls occur with some data followed by the closing of the handle/descriptor. All these calls are grouped into one event, namely the writing of a file.\\


The events generated by the visiting of the URLs are put into a directed acyclic graph (DAG). Causally related events are connected by an edge. Say, for example, that a webpage is cached by a browser. Then the cached version of that webpage is stored on a disk. The visiting of that webpage and the following cache write are causally related and hence a directed edge should be created between those two events.\\

Although no specific structure of the DAG is required, a tree like structure seems the most fitting for the problem of tracking events to a browsing context. In this structure each browsing context has its own subtree of events. 



\subsubsection{Step 3: Analyze}

After the graph has been made, analysis algorithms can be run on the graph. It's important to note that no analysis is done in Step 2, all events are put in the graph. It's up to the analyzers in this step to find malicious behavior. Analyzers are of course highly dependant on the graph structure to correctly interpret the graph.\\

%It must be noted that analysis might be done every time a browsing context was closed, allowing for faster reporting of malware.
%It must be noted that analysis might be done whilst creating the graph but it should not influence the creating of the graph, neither should it alter the graph in any way.

\subsubsection{Step 4: Report}

The final step is reporting to the user. After Step 3, each analyzer reports its findings back to the user. It goes without saying that the analyzer should give clear and precise information of what happened.
