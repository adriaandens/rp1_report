For a better insight into the project, it is necessary to introduce certain theoretical concepts first. The next chapter will use this theory as a basis for the design and development of the algorithm.

\subsection{Drive-by downloads}

Browsing the internet with an unpatched system can be dangerous. All software contains mistakes and fixes are released on a daily basis. Part of those mistakes can be (ab)used to get control over a computer system and be used to run malicious software without the user noticing. Such mistakes are called vulnerabilities.

If such vulnerability is used to take control over the web browser (or one of its plugins like Flash or Java) and download malicious software to the system, then this is called a drive-by download\cite{Le2013}. Figure \ref{fig:dbdownload} shows the steps involved from visiting a website until the moment the system is infected with malware.

By compromising the web browser and injecting malicious code (called exploitation), the malware gets full control over the infected process, running with the same privileges as the web browser on the host system. Depending on the system configuration this either means that the system is now under the control of the attacker or that additional steps are required to escalate the privileges to the intended level.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{Images/drive-by-download.png}
    \caption{The anatomy of a drive-by download malware infection. \cite{dbdownload-anatomy}}
    \label{fig:dbdownload}
\end{figure}

\subsubsection{Behaviour}
\label{sec:behavior}

What happens after a malware infection depends on the malware and the goals of the attacker. Common goals are persistence of the malware and further exploitation. The former is achieved by writing executable code to the disk and optionally changing several configuration files to make sure the code is executed at certain events, for example after reboot. The latter is achieved by dowloading further malicious components for the next stage. We thus expect to see filesystem operations and additional network traffic after the first exploit.

While some malware communicates directly with the kernel (via so-called system calls), most malware \cite{MaliciousAPIs} behaves like a normal application and uses the installed, or with the operating system provided, libraries. The usage of such libraries can be detected when the access to them is monitored.

A special class of malware \cite{ZeroAccess,Gameover} is formed by those that infiltrate the operating system kernel by loading malicious drivers. Such malware is called a rootkit and is very hard to detect and analyse as it runs with the highest possible priviliges. This allows it to hide itselfs, files, network connections and other applications for the user and normal applications from the moment the driver is loaded.

\subsection{Libraries and APIs}

Libraries contain reusable parts of code to make the development of applications easier. Applications use them so the developer does not have to worry about the low-level details of the used system. The application programming interface (API) defines the exposed interface of the library with the functions that can be used.

In the early days, libraries were primarily used by an application by statically linking to it. This means that the library becomes part of the application and it is no longer possible to determine which part of the application was originally part of the used libraries.

For performance and maintainability, dynamic linking was invented. The application describes which libraries it needs and the linker of the operating system will glue the applications and its dependent libraries (which are defined in the import section of the application) together in the memory space of the application. This happens at runtime. 

Because the linker has to know all exported functions and their location, a symbol table is part of every dynamic library. The same information can be used to hook into a function during runtime or to trick the linker in loading a replacement for a function. This is called API hooking \cite{APIHookingRevealed}.

\subsubsection{API hooking}

API hooking is a widely used technique to monitor or change the behaviour of applications. With API hooking, the original function is replaced by a substitute (or hook). This substitute function, for example, first logs the performed operation and then calls the original function. Or the substitute could be a custom replacement of the original function.

The technical implementation of API hooking is highly complex and platform specific. Many different techniques\cite{jbremer2012} of hooking are possible as well. If the start of the application can be controlled, the linker search path can be extended to include the replacement library. Alternatively, the import section of the application can be modified. When the application is already loaded or modification of the application or system is undesired, the function to be hooked can be overwritten in memory with a replacement or jump to a different location in memory as can be seen in figure \ref{fig:apihooking}. However, this will prevent the ability to execute the original function unless the overwritten bytes are carefully preserved and reconstructed somewhere else (a so-called trampoline).

\begin{figure}
    \centering
    \includegraphics[width=14.7cm]{Images/API-hooking.png}
    \caption{Example of how an API function can be hooked at runtime by overwriting the first bytes with a jump to a substitue function. If the original function has to be preserved, an trampoline is added which consists of the overwritten bytes of the original function. \cite{APIHookImage}}
    \label{fig:apihooking}
\end{figure}

In this project API hooking will be used to reverse engineer the internal workings and API usage of web browsers and to log the behaviour of the web browser and malware for later analysis.\todo{Verplaatsen naar approach?}

\subsubsection{Alternatives}



\subsection{Web browser architecture}
\input{Chapters/04.1_Web_Browser_Arch.tex}

\subsection{Correlating HTTP requests}

\todo{Verplaatsen}

The challenge faced when multiple websites have to be loaded at the same time, is to know which HTTP request corresponds to which website. Many webpages consist of dozens of resources that have to be loaded. The loading of some resources can even be delayed until after certain predefined events. In some libraries, the requesting of a web resource consists of several independant steps.

An easy solution would be to modify the web browser in such a way that it exposes this information with an easy to use interface. While this would solve the problem, regular maintenance would be required to keep this system working. 

A solution could be to log all the network traffic and analyse it. While this information is always available, it would require complex protocol and content parsers to reconstruct the original network streams and extract useful information from it. Additionally, an encrypted connection would require a proxy that uses on-the-fly creation of certificates. Even then it would be trivial to circumvent this system as scripting languages and browser plugins could be used to dynamically request resources.

A better solution would be to correlate a HTTP request to its originating webpage by observing the behaviour and environment of the web browser. By logging the API calls that are made by the web browser, the full process and thread context of every call is available or can be reconstructed from earlier calls. As all modern web browsers use high-level network libraries, this is the ideal place to monitor. When combined with other interesting APIs, such as file, registry, low-level network or process management operations, a full insight in the behaviour of the web browser is available and detecting malicious behaviour is a matter of writing the correct behavioural analysers.

\todo{Plaatje cuckoomon}
