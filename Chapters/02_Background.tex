For a better insight into the project, it is necessary to introduce certain theoretical concepts first. The next chapter will use this theory as a basis for the design and development of the algorithm.

\subsection{Drive-by downloads}

Browsing the internet with an unpatched system can be dangerous. Many, if not all, software contains mistakes and such mistakes are patched almost on a daily basis. Part of those mistakes can be (ab)used to get control over a computer system and be used to run malicious software without the user noticing.

If a website uses such a mistake to take control over the web browser and download malicious software to the system, then this is called a drive-by download\cite{Le2013}. Figure \ref{fig:dbdownload} shows the steps involved from visiting a website until the moment the system is infected with malware.

By compromising the web browser and injecting malicious code, the malware gets full control over the infected process, running with the same privileges as the web browser on the host system. Depending on the system configuration this either means that full system access is available or that additional steps are required to escalate the privileges to the intended level.


\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{Images/drive-by-download.png}
    \caption{The anatomy of a drive-by download malware infection. \cite{dbdownload-anatomy}}
    \label{fig:dbdownload}
\end{figure}

\subsubsection{Behaviour}
\label{sec:behavior}

What happens after a malware infection depends on the malware and the goals of the attacker. But usually the goals of the attacker are persistence and further exploitation. The former is achieved by writing executable code to the disk and optionally changing several configuration files to make sure the code is executed at certain events, for example after reboot. The latter is achieved by dowloading further malicious components. We thus expect to see extra network traffic after the first exploit.

While theoretically malware could directly communicate with the kernel, most malware\todo{need citation} behaves like a normal application and uses the installed, or with the operating system provided, libraries. The usage of such libraries can be detected when the access to them is monitored.

\subsection{API hooking}

In the early days, libraries were primarily used by an application by statically linking to it. This means that the library becomes part of the application and it is no longer possible to determine which part of the application was originally part of the used libraries.

For performance and maintainability, dynamic linking was invented. The application describes which libraries it needs and the linker of the operating system will glue the applications and its dependent libraries together in the memory space of the application. This happens at runtime. 

Because the linker has to know all exported functions and their location, a symbol table is part of every dynamic library. The same information can be used to hook into a function during runtime or to trick the linker in loading a replacement for a function.

This is called API hooking\cite{wikihooking} and it is a widely used technique to monitor the behaviour of applications. With API hooking, the original function is replaced by a substitute. This substitute function, for example, first logs the performed operation and then calls the original function. Or the substitute could be a custom replacement of the original function.

The technical implementation of API hooking is highly complex and platform specific. Many different techniques\cite{jbremer2012} of hooking are possible as well. If the start of the application can be controlled, the linker search path can be extended to include the replacement library. Alternatively, the import section of the application can be modified. When the application is already loaded or modification of the application or system is unwanted, the function to be hooked can be overwritten in memory with a replacement or jump to a different location in memory. However, this will prevent the ability to execute the original function unless the overwritten bytes are carefully preserved and reconstructed somewhere else.

In this project API hooking will be used to reverse engineer the internal workings and API usage of web browsers and to log the behaviour of the web browser and malware for later analysis.

\subsection{Web browser architecture}
\input{Chapters/04.1_Web_Browser_Arch.tex}

\subsection{Correlating HTTP requests}

The challenge faced when multiple websites have to be loaded at the same time, is to know which HTTP request corresponds to which website. Many webpages consist of dozens of resources that have to be loaded. The loading of some resources can even be delayed until after certain predefined events. In some libraries, the requesting of a web resource consists of several independant steps.

An easy solution would be to modify the web browser in such a way that it exposes this information with an easy to use interface. While this would solve the problem, regular maintenance would be required to keep this system working. 

A solution could be to log all the network traffic and analyse it. While this information is always available, it would require complex protocol and content parsers to reconstruct the original network streams and extract useful information from it. Additionally, an encrypted connection would require a proxy that uses on-the-fly creation of certificates. Even then it would be trivial to circumvent this system as scripting languages and browser plugins could be used to dynamically request resources.

A better solution would be to correlate a HTTP request to its originating webpage by observing the behaviour and environment of the web browser. By logging the API calls that are made by the web browser, the full process and thread context of every call is available or can be reconstructed from earlier calls. As all modern web browsers use high-level network libraries, this is the ideal place to monitor. When combined with other interesting APIs, a full insight in the behaviour of the web browser is available and detecting malicious behaviour is a matter of writing the correct behavioural analysers.
