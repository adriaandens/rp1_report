This section discusses the implementation of the algorithm, the problems encountered and a comparison with the latest Cuckoo version.

\subsection{Implementing the algorithm}

What follows is a short overview of how the algorithm was implemented, the actual code can be found on GitHub\footnote{\url{https://github.com/MartijnB/cuckoo/tree/multi-url}}.

\begin{enumerate}
\item \textbf{Visit} To support the parallel visiting of URLs in one virtual machine, Cuckoo had to be extended. Support for this was added using tabs, but this left us with a problem to detect when a new URL was feeded to a tab (see \ref{99problems}). To solve this problem, every URL in this PoC is now opened in its own window.

\item \textbf{Process} In this step, the API calls were bundled into ten different events before being added to the graph. If no relation was found between an event and a previous event, an edge was created between the event itself and the event which represents the browser context. The HTTP `Referer' header was used to find relations between HTTP events, essentially creating a referer tree \cite{qui}. %\todo{Discuss all relations here?}

\item \textbf{Analyse} To implement the analysis phase of the algorithm, a simple analyser was written that detects process spawns below browsing contexts. Appendix A shows pseudo code of the analyser.

\item \textbf{Report} Reporting is done on the commandline but generated graphs of anomalities are saved to the disk in the folder structure of Cuckoo. The raw data that was generated by the virtual machine, is also retained in the Cuckoo folder structure.
\end{enumerate}

\subsubsection{Problems}
\label{99problems}
%\epigraph{I've got 99 problems but Cuckoo ain't one.}{Adriaan}

\textbf{Opening a new URL in the same browser context} was not detectable by the monitored API calls. On top of that, processes were reused when a browsing context was closed and a new one was opened, this made it essentially the same as reusing the browsing context. Internet Explorer also behaves differently when interacting with the automation interface (COM) compared to real user interaction, leaving us without distinct API calls that could be used to detect the opening of a new URL. Therefor we decided to use a new Internet Explorer process for each URL which gives us a process ID per browser context.

\textbf{Working with JSON} was extremely slow in Cuckoo. After the virtual machine has done its work, BSON files are transferred to the host machine. These BSON files are then parsed by Cuckoo and analysed after which a JSON file is written. Our \texttt{mass-analyse.py} depended on this JSON, which made it very slow to use. A BSON parser was written to skip the whole JSON step and thus we were able to work earlier on the data, giving us a significant speedup.

\subsection{Running the PoC}

Running the proof of concept is as simple as running Cuckoo and running our Python script. Listing \ref{code:run} shows how the PoC is run. As explained in section \ref{sec:setup} the URL list contains the Top 20 most visited websites and one website with malware. Figure \ref{fig:graph} shows the full graph created in the process phase. Notice the red dots in the top left corner of the graph which indicate process spawns and the purple dots which indicate shell command executions. The analyser run in phase 3 successfully found this anomality and reports it back to the user. A graph of the browsing context in which this anomality occurred is also shown to the user, as can be seen in figure \ref{fig:subgraph}.

\pagebreak

\begin{lstlisting}[caption={Mass analyser being run},label={code:run}]
$ python cuckoo.py &
$ python utils/mass-analyse.py url_list.txt
Warning: Task with ID 22 is not yet completed; Waiting...
INFO:root:Parse log....
[...]
PID 2876 'iexplore.exe' spawned from parent PID 2860
Visiting: http://google.com/
PID 3656 'iexplore.exe' spawned from parent PID 2860
Visiting: http://unsuspicious.com/
PID 2108 'iexplore.exe' spawned from parent PID 2860
Visiting: http://google.nl/
PID 3064 'iexplore.exe' spawned from parent PID 2860
Visiting: http://imdb.com/
PID 1012 'iexplore.exe' spawned from parent PID 2860
Visiting: http://facebook.com/
PID 3728 'control.exe' spawned from parent PID 3656
PID 2848 'repfix.exe' spawned from parent PID 3656
PID 1944 'rundll32.exe' spawned from parent PID 3728
PID 3780 'ynuni.exe' spawned from parent PID 2848
[...]
Analyser 'Subprocess_from_tab': The URL 'http://unsuspicious.com' spawns a process called 'control.exe', 'repfix.exe', 'rundll32.exe' and     'ynuni.exe'.
\end{lstlisting}

\setcounter{savepage}{\arabic{page}}
\stepcounter{savepage}
\pagebreak

\pagenumbering{gobble}
\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=20cm]{Images/graph4.jpg}}
    \caption{An example of the graph generated by visiting the Alexa top 20 and one malicious website. The arrows indicate malicious events generated by the malware.}
    \label{fig:graph}
\end{figure}

\stepcounter{savepage}
\pagebreak

\newgeometry{left=3cm,top=0.1cm,bottom=0.1cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=25cm, angle=90]{Images/report_Subprocess_from_tab}
    \caption{An example of the subgraph where a single website was responsible for malware. For clarity, only the labels of visited URLs, involved processes and executed shell commands are shown.}
    \label{fig:subgraph}
\end{figure}

\stepcounter{savepage}
\pagebreak

\restoregeometry
\pagenumbering{arabic}
\setcounter{page}{\thesavepage}

\subsection{Comparison with other malware analysis systems}

To quantify the improvement that was made, several benchmarks were run against our improved version, henceforth called ``Roadrunner''\footnote{http://en.wikipedia.org/wiki/Geococcyx}. Those benchmarks were compared with a development snapshot of Cuckoo 1.2\footnote{\url{https://github.com/cuckoobox/cuckoo/commit/6177071cfd57500fbf1dc17a66f5aff39051c75e}} and Anubis. %, two malware analysis systems which visit URLs sequentially.% The development version of Cuckoo was also used as the base for our development.<-- dit is niet de goeie plaats om dat te zeggen....

The benchmarks consists of provisioning the system with one or more URLs. With Cuckoo, the time is measured until the status changes to ``reported''. Anubis reports contain a parameter called ``time needed'' which was used as the benchmark time. The Roadrunner benchmark measures the time between submitting the URL list and the exit of the developped analysis script. Each benchmark was run 25 times to filter out anomalities.

For Roadrunner, benchmarks were executed with 1, 5, 10, 25, 50 and 100 URLs. If the benchmark crashed (or hit the critical time-out) the measurement was thrown away as this is a known issue\footnote{The monitor component of Cuckoo is under active development and a significant upgrade is expected in the coming months.}, but no easy workaround was available. Because Anubis is a hosted system outside our control, it was not possible to run the benchmark with high(er) URL counts. For this reason a maximum of 10 URLs at the same time is used. For the unmodified Cuckoo was not enough time available to benchmark with 50 and 100 URLs at the same time. Based on the fact that both systems visit the URLs sequentially and the benchmarks show an (almost) liniair increase, extrapolation is possible with an acceptable margin of error.

Table \ref{tbl:results} shows the summary\footnote{The raw numbers used to generate the table and graphs can be found in Appendix B.} of the benchmarks. This table gives mean time over 25 runs with a certain amount of URLs. A rough comparison in the speed between Cuckoo and Roadrunner has also been made. Although the difference is significant, a important sidenote must be made that speed is not the primary goal of Cuckoo and that Cuckoo, if wanted, can be speeded up by configuration tweaks. The changes made to speedup the proof of concept removed a lot of the detailed insights Cuckoo gives in the behaviour of malware. For this project, however, only the data that makes it possible to detect malicious activity is of importance, not detailed insight of the exact behaviour of malware.

Figure \ref{fig:chart-box} and figure \ref{fig:chart-trend} show these numbers in a different way. Figure \ref{fig:chart-box} shows the boxplots of the different runs of Roadrunner. We can say that for a higher amount of URLs, there's bigger variance in the time it takes to complete. It was not expected this would result in such (relatively) big spreading and there is no satisfactorily explanation for this behaviour. The expected result was that the differences between websites would even out on higher URL counts. Figure \ref{fig:chart-trend} is a visual representation of table \ref{tbl:results}, on the x-axis we see the number of URLs; the y-axis the time it takes to analyse these URLs. 

\setcounter{savepage}{\arabic{page}}
\stepcounter{savepage}
\pagebreak

\pagenumbering{gobble}
\newgeometry{left=3cm,top=2.5cm,bottom=0.1cm}

\begin{figure}[h!]
    \centering
    \centerline{\includegraphics[width=15cm]{Images/chart-box.png}}
    \caption{A boxplot of the Roadrunner benchmark durations. A higher URL count results in a higher variance of the duration.}
    \label{fig:chart-box}
\end{figure}

\begin{table}[h]
\begin{tabular}{@{}lllllll@{}}
\toprule
                                  & 1 URL                       & 5 URLs                     & 10 URLs                     & 25 URLs                     & 50 URLs                     & 100 URLs \\ \midrule
Anubis                            & \multicolumn{1}{r}{274s}    & \multicolumn{1}{r}{1397s}  & \multicolumn{1}{r}{2793s}   & \multicolumn{1}{r}{-}       & \multicolumn{1}{r}{-}       & \multicolumn{1}{r}{-}      \\                                  
Cuckoo                            & \multicolumn{1}{r}{152,8s}  & \multicolumn{1}{r}{769,7s} & \multicolumn{1}{r}{1575,2s} & \multicolumn{1}{r}{tbd}     & \multicolumn{1}{r}{-}       & \multicolumn{1}{r}{-}      \\
Roadrunner                        & \multicolumn{1}{r}{48,8s}   & \multicolumn{1}{r}{74,8s}  & \multicolumn{1}{r}{102,4s}  & \multicolumn{1}{r}{160,1s}  & \multicolumn{1}{r}{286,4s}  & \multicolumn{1}{r}{450,9s} \\
Improvement                       & \multicolumn{1}{r}{3-6x}    & \multicolumn{1}{r}{14-27x} & \multicolumn{1}{r}{33-60x}  \\ \bottomrule
\end{tabular}
\caption{Mean runtime in seconds of Cuckoo, Roadrunner and Anubis. Limitations of available time prevented the benchmarking of Anubis and Cuckoo with higher URL counts.}
\label{tbl:results}
\end{table}

\begin{figure}[h!]
    \centering
    \centerline{\includegraphics[width=15cm]{Images/chart-trend}}
    \caption{A line chart with the results of the benchmarks. To give an indication how long it would take for higher amounts of URLs, the data of Anubis and Cuckoo have been extrapolated for up to 100 URLs. Roadrunner is in all cases faster than both existing systems which run sequentially.}
    \label{fig:chart-trend}
\end{figure}

\stepcounter{savepage}
\pagebreak

\restoregeometry
\pagenumbering{arabic}
\setcounter{page}{\thesavepage}
